{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94eb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Flatten\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "DATA_PATH = \"/home/smayan/Desktop/ASL/dataset/pk-hfad-1.landmarks-mediapipe-world-csv/\"\n",
    "JSON_PATH = \"./pk-dictionary-mapping.json\"\n",
    "MAX_SEQ_LENGTH = 100\n",
    "NUM_FEATURES = None\n",
    "\n",
    "print(\"Step 1: Loading labels and creating mappings...\")\n",
    "\n",
    "with open(JSON_PATH, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "all_labels = []\n",
    "for entry in json_data:\n",
    "    if 'mapping' in entry:\n",
    "        for item in entry['mapping']:\n",
    "            if 'label' in item:\n",
    "                all_labels.append(item['label'])\n",
    "\n",
    "unique_labels = sorted(list(set(all_labels)))\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(unique_labels)\n",
    "NUM_CLASSES = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Found {len(unique_labels)} unique labels.\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "\n",
    "print(\"\\nStep 2: Loading landmark data from CSV files...\")\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "available_files = os.listdir(DATA_PATH)\n",
    "\n",
    "for filename in available_files:\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_label = os.path.splitext(filename)[0]\n",
    "        \n",
    "        if file_label in label_encoder.classes_:\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(DATA_PATH, filename), header=None)\n",
    "                \n",
    "                if NUM_FEATURES is None:\n",
    "                    NUM_FEATURES = df.shape[1]\n",
    "                    print(f\"Dynamically determined number of features: {NUM_FEATURES}\")\n",
    "\n",
    "                sequences.append(df.values)\n",
    "                labels.append(label_encoder.transform([file_label])[0])\n",
    "            except Exception as e:\n",
    "                print(f\"Could not read or process file {filename}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(sequences)} sequences.\")\n",
    "\n",
    "if not sequences:\n",
    "    raise ValueError(\"No sequences were loaded. Check the DATA_PATH and file contents.\")\n",
    "    \n",
    "print(\"\\nStep 3: Preprocessing data (padding and encoding)...\")\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post', dtype='float32')\n",
    "y = to_categorical(labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "print(f\"Shape of data tensor X: {X.shape}\")\n",
    "print(f\"Shape of labels tensor y: {y.shape}\")\n",
    "\n",
    "print(\"\\nStep 4: Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "print(\"\\nStep 5: Building the Hybrid CNN-LSTM model...\")\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(MAX_SEQ_LENGTH, NUM_FEATURES)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(128, return_sequences=True, activation='relu'),\n",
    "    LSTM(64, return_sequences=False, activation='relu'),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"\\nStep 6: Compiling the model...\")\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print(\"\\nStep 7: Starting model training...\")\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "print(\"\\nStep 8: Evaluating model performance on the test set...\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "\n",
    "# model.save('sign_language_cnn_lstm.h5')\n",
    "# print(\"Model saved as 'sign_language_cnn_lstm.h5'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
