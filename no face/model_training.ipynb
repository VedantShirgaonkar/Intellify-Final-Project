{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11371998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, TimeDistributed, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454d00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6efd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b943e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6df4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([ lh, rh])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21f187b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/smayan/Desktop/ASL/dataset/SL'\n",
    "sequence_length = 30\n",
    "min_sequences_per_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e043d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = [\n",
    "#     'a', 'about', 'again', 'all', 'also', 'always', 'and', 'angry', 'animal', 'answer', \n",
    "#     'apple', 'ask', 'baby', 'bad', 'bathroom', 'beautiful', 'because', 'bed', 'before', \n",
    "#     'big', 'book', 'boy', 'brother', 'but', 'buy', 'bye', 'call', 'can', 'car', 'cat', \n",
    "#     'city', 'class', 'clean', 'clothes', 'cold', 'college', 'color', 'come', 'computer', \n",
    "#     'cook', 'dad', 'day', 'deaf', 'different', 'doctor', 'dog', 'done', \"don't want\", \n",
    "#     'down', 'drink', 'eat', 'eight', 'enough', 'family', 'fast', 'father', 'feel', \n",
    "#     'find', 'fine', 'finish', 'first', 'five', 'food', 'for', 'four', 'friend', 'from', \n",
    "#     'get', 'girl', 'give', 'go', 'good', 'goodbye', 'happy', 'hard', 'have', \n",
    "#     'head', 'hearing', 'hello', 'help', 'her', 'here', 'home', 'hospital', 'hot', \n",
    "#     'house', 'how', 'hungry', 'i', 'if', 'in', 'know', 'language', 'last', 'later', \n",
    "#     'learn', 'letter', 'like', 'little bit', 'live', 'look at', 'love', 'make', 'man', \n",
    "#     'many', 'me', 'meet', 'milk', 'mom', 'money', 'month', 'more', 'morning', 'mother', \n",
    "#     'movie', 'music', 'my', 'name', 'need', 'never', 'new', 'nice', 'night', 'nine', \n",
    "#     'no', 'not', 'now', 'old', 'on', 'one', 'open', 'orange', 'our', 'out', 'people', \n",
    "#     'phone', 'play', 'please', 'put', 'question', 'read', 'ready', 'red', 'right', 'sad', \n",
    "#     'same', 'say', 'school', 'see', 'seven', 'she', 'shirt', 'shoes', 'show', 'sick', \n",
    "#     'sign', 'sign language', 'sister', 'sit', 'six', 'sleep', 'slow', 'small', 'sorry', \n",
    "#     'stand', 'start', 'stop', 'store', 'story', 'student', 'study', 'talk', 'teach', \n",
    "#     'teacher', 'tell', 'ten', 'thank you', 'that']\n",
    "# # 'the', 'their', 'they', 'thing', \n",
    "# #     'think', 'thirsty', 'this', 'three', 'time', 'tired', 'to', 'today', 'tomorrow', \n",
    "# #     'two', 'understand', 'up', 'use', 'wait', 'walk', 'want', 'water', 'way', \n",
    "# #     'we', 'wear', 'week', 'what', 'when', 'where', 'which', 'white', 'who', 'why', \n",
    "# #     'will', 'with', 'woman', 'word', 'work', 'world', 'write', 'wrong', 'year', 'yellow', \n",
    "# #     'yes', 'yesterday', 'you', 'your'\n",
    "# # ]\n",
    "# label_map = {label: num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d7eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = ['hello', 'student','i','bye','goodbye','college','wrong','how', 'work', 'your', 'want', 'nice', 'to', 'meet', 'doctor', 'time', 'age', 'breakfast', 'sorry', 'love']\n",
    "actions = [\"Hello\",\"Help\",\"How are you?\", \"See you later\"]\n",
    "label_map = {label: num for num, label in enumerate(actions)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22686414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57fc233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7801fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv.VideoCapture('/home/smayan/Desktop/ASL/dataset/SL/wood/63723.mp4')\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             print(\"Video ended or cannot read the frame.\")\n",
    "#             break\n",
    "\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "            \n",
    "\n",
    "#         draw_styled_landmarks(image, results)\n",
    "\n",
    "#         cv.imshow('OpenCV feed', image)\n",
    "#         if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "# cap.release()\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b99241aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('/home/smayan/Desktop/ASL/dataset/custom/X.npy')\n",
    "y = np.load('/home/smayan/Desktop/ASL/dataset/custom/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "853036dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30, 126)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb739f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82c6ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.shape[2]\n",
    "X = X.reshape(X.shape[0], X.shape[1], num_features, 1)\n",
    "\n",
    "y_categorical = to_categorical(y, num_classes=len(actions))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35120218",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "perm = np.random.permutation(len(X_train))\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2264757",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.squeeze(-1)\n",
    "X_test  = X_test.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51056d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 30, 126, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5ff6911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 30, 126)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e5e9153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3206bdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "821d602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights computed: {0: np.float64(1.0), 1: np.float64(1.0), 2: np.float64(1.0), 3: np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class weights computed: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "908e16ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 21:10:17.419093: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/smayan/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(sequence_length, X.shape[2])))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(sequence_length, X.shape[2])))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(actions), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a796af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f'logs/wsl_model_{timestamp}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "677d1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66e8fc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">261,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m261,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m164,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m132\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">430,756</span> (1.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m430,756\u001b[0m (1.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">430,244</span> (1.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m430,244\u001b[0m (1.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22367f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 30, 126)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80cc6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "MONITOR_METRIC = 'val_accuracy'\n",
    "MONITOR_MODE = 'max' \n",
    "\n",
    "log_dir = \"/home/smayan/Desktop/ASL/logs\"\n",
    "\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='best_asl_model.keras',\n",
    "    save_weights_only=False,        \n",
    "    monitor=MONITOR_METRIC,\n",
    "    mode=MONITOR_MODE,\n",
    "    save_best_only=True,            \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=MONITOR_METRIC,\n",
    "    patience=20,  \n",
    "    verbose=1,\n",
    "    mode=MONITOR_MODE,\n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor=MONITOR_METRIC,\n",
    "    factor=0.2,\n",
    "    patience=10,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1,\n",
    "    mode=MONITOR_MODE\n",
    ")\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    model_checkpoint_callback, # <-- ADDED\n",
    "    early_stopping_callback,   # <-- MODIFIED\n",
    "    reduce_lr_callback       # <-- MODIFIED\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "263b751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.1875 - loss: 1.4558\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25000, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 291ms/step - accuracy: 0.3268 - loss: 1.3531 - val_accuracy: 0.2500 - val_loss: 1.2883 - learning_rate: 0.0010\n",
      "Epoch 2/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5625 - loss: 0.9813\n",
      "Epoch 2: val_accuracy improved from 0.25000 to 0.29167, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6367 - loss: 0.9068 - val_accuracy: 0.2917 - val_loss: 1.3008 - learning_rate: 0.0010\n",
      "Epoch 3/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8125 - loss: 0.6394\n",
      "Epoch 3: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8359 - loss: 0.6364 - val_accuracy: 0.2917 - val_loss: 1.3601 - learning_rate: 0.0010\n",
      "Epoch 4/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8125 - loss: 0.5675\n",
      "Epoch 4: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8086 - loss: 0.5864 - val_accuracy: 0.2500 - val_loss: 1.3767 - learning_rate: 0.0010\n",
      "Epoch 5/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9375 - loss: 0.4100\n",
      "Epoch 5: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8945 - loss: 0.4354 - val_accuracy: 0.2500 - val_loss: 1.3707 - learning_rate: 0.0010\n",
      "Epoch 6/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8125 - loss: 0.3728\n",
      "Epoch 6: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8841 - loss: 0.3532 - val_accuracy: 0.2500 - val_loss: 1.3932 - learning_rate: 0.0010\n",
      "Epoch 7/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9062 - loss: 0.2866\n",
      "Epoch 7: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8867 - loss: 0.3090 - val_accuracy: 0.2500 - val_loss: 1.4345 - learning_rate: 0.0010\n",
      "Epoch 8/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9688 - loss: 0.2733\n",
      "Epoch 8: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9688 - loss: 0.2283 - val_accuracy: 0.2500 - val_loss: 1.4191 - learning_rate: 0.0010\n",
      "Epoch 9/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9688 - loss: 0.1788\n",
      "Epoch 9: val_accuracy did not improve from 0.29167\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9596 - loss: 0.1796 - val_accuracy: 0.2500 - val_loss: 1.3031 - learning_rate: 0.0010\n",
      "Epoch 10/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9375 - loss: 0.1916\n",
      "Epoch 10: val_accuracy improved from 0.29167 to 0.37500, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9609 - loss: 0.1665 - val_accuracy: 0.3750 - val_loss: 1.3095 - learning_rate: 0.0010\n",
      "Epoch 11/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0964\n",
      "Epoch 11: val_accuracy improved from 0.37500 to 0.50000, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9805 - loss: 0.1278 - val_accuracy: 0.5000 - val_loss: 1.2317 - learning_rate: 0.0010\n",
      "Epoch 12/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9688 - loss: 0.1696\n",
      "Epoch 12: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9596 - loss: 0.1471 - val_accuracy: 0.5000 - val_loss: 1.2495 - learning_rate: 0.0010\n",
      "Epoch 13/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9688 - loss: 0.0677\n",
      "Epoch 13: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9557 - loss: 0.0915 - val_accuracy: 0.5000 - val_loss: 1.1856 - learning_rate: 0.0010\n",
      "Epoch 14/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9375 - loss: 0.1154\n",
      "Epoch 14: val_accuracy improved from 0.50000 to 0.58333, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9388 - loss: 0.1283 - val_accuracy: 0.5833 - val_loss: 0.9161 - learning_rate: 0.0010\n",
      "Epoch 15/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9062 - loss: 0.1583\n",
      "Epoch 15: val_accuracy improved from 0.58333 to 0.62500, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9401 - loss: 0.1320 - val_accuracy: 0.6250 - val_loss: 0.9849 - learning_rate: 0.0010\n",
      "Epoch 16/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0347\n",
      "Epoch 16: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0507 - val_accuracy: 0.5833 - val_loss: 1.0864 - learning_rate: 0.0010\n",
      "Epoch 17/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0399\n",
      "Epoch 17: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9896 - loss: 0.0603 - val_accuracy: 0.5417 - val_loss: 1.1106 - learning_rate: 0.0010\n",
      "Epoch 18/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0610\n",
      "Epoch 18: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0660 - val_accuracy: 0.4583 - val_loss: 0.8202 - learning_rate: 0.0010\n",
      "Epoch 19/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0316\n",
      "Epoch 19: val_accuracy improved from 0.62500 to 0.75000, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9857 - loss: 0.0590 - val_accuracy: 0.7500 - val_loss: 0.6425 - learning_rate: 0.0010\n",
      "Epoch 20/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9688 - loss: 0.0920\n",
      "Epoch 20: val_accuracy did not improve from 0.75000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9779 - loss: 0.0735 - val_accuracy: 0.7500 - val_loss: 0.5319 - learning_rate: 0.0010\n",
      "Epoch 21/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0414\n",
      "Epoch 21: val_accuracy did not improve from 0.75000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9896 - loss: 0.0523 - val_accuracy: 0.6250 - val_loss: 0.6411 - learning_rate: 0.0010\n",
      "Epoch 22/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9688 - loss: 0.0889\n",
      "Epoch 22: val_accuracy did not improve from 0.75000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9831 - loss: 0.0630 - val_accuracy: 0.6250 - val_loss: 0.7784 - learning_rate: 0.0010\n",
      "Epoch 23/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9688 - loss: 0.0949\n",
      "Epoch 23: val_accuracy did not improve from 0.75000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9831 - loss: 0.0746 - val_accuracy: 0.6667 - val_loss: 0.6573 - learning_rate: 0.0010\n",
      "Epoch 24/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9688 - loss: 0.0532\n",
      "Epoch 24: val_accuracy did not improve from 0.75000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9831 - loss: 0.0476 - val_accuracy: 0.7083 - val_loss: 0.5023 - learning_rate: 0.0010\n",
      "Epoch 25/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0145\n",
      "Epoch 25: val_accuracy improved from 0.75000 to 0.79167, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.0250 - val_accuracy: 0.7917 - val_loss: 0.4070 - learning_rate: 0.0010\n",
      "Epoch 26/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0417\n",
      "Epoch 26: val_accuracy improved from 0.79167 to 0.95833, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9948 - loss: 0.0362 - val_accuracy: 0.9583 - val_loss: 0.3238 - learning_rate: 0.0010\n",
      "Epoch 27/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0141\n",
      "Epoch 27: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0160 - val_accuracy: 0.9583 - val_loss: 0.3099 - learning_rate: 0.0010\n",
      "Epoch 28/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0584\n",
      "Epoch 28: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9909 - loss: 0.0471 - val_accuracy: 0.9167 - val_loss: 0.3728 - learning_rate: 0.0010\n",
      "Epoch 29/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9688 - loss: 0.0656\n",
      "Epoch 29: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9831 - loss: 0.0496 - val_accuracy: 0.6667 - val_loss: 0.7154 - learning_rate: 0.0010\n",
      "Epoch 30/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0479\n",
      "Epoch 30: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0409 - val_accuracy: 0.4167 - val_loss: 1.1036 - learning_rate: 0.0010\n",
      "Epoch 31/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0471\n",
      "Epoch 31: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0357 - val_accuracy: 0.5417 - val_loss: 0.8330 - learning_rate: 0.0010\n",
      "Epoch 32/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0377\n",
      "Epoch 32: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9909 - loss: 0.0391 - val_accuracy: 0.6250 - val_loss: 0.6968 - learning_rate: 0.0010\n",
      "Epoch 33/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9688 - loss: 0.0478\n",
      "Epoch 33: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9831 - loss: 0.0420 - val_accuracy: 0.7500 - val_loss: 0.6324 - learning_rate: 0.0010\n",
      "Epoch 34/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9688 - loss: 0.0527\n",
      "Epoch 34: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9831 - loss: 0.0374 - val_accuracy: 0.6667 - val_loss: 0.7630 - learning_rate: 0.0010\n",
      "Epoch 35/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0254\n",
      "Epoch 35: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9818 - loss: 0.0866 - val_accuracy: 0.8750 - val_loss: 0.4448 - learning_rate: 0.0010\n",
      "Epoch 36/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0680\n",
      "Epoch 36: val_accuracy did not improve from 0.95833\n",
      "\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9896 - loss: 0.0927 - val_accuracy: 0.8333 - val_loss: 0.5045 - learning_rate: 0.0010\n",
      "Epoch 37/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9375 - loss: 0.1273\n",
      "Epoch 37: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9518 - loss: 0.1018 - val_accuracy: 0.8333 - val_loss: 0.4406 - learning_rate: 2.0000e-04\n",
      "Epoch 38/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0441\n",
      "Epoch 38: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9909 - loss: 0.0509 - val_accuracy: 0.8750 - val_loss: 0.3890 - learning_rate: 2.0000e-04\n",
      "Epoch 39/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0331\n",
      "Epoch 39: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9948 - loss: 0.0504 - val_accuracy: 0.8750 - val_loss: 0.3424 - learning_rate: 2.0000e-04\n",
      "Epoch 40/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9688 - loss: 0.0679\n",
      "Epoch 40: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9779 - loss: 0.0534 - val_accuracy: 0.9167 - val_loss: 0.3057 - learning_rate: 2.0000e-04\n",
      "Epoch 41/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0429\n",
      "Epoch 41: val_accuracy improved from 0.95833 to 1.00000, saving model to best_asl_model.keras\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0356 - val_accuracy: 1.0000 - val_loss: 0.2752 - learning_rate: 2.0000e-04\n",
      "Epoch 42/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9688 - loss: 0.0751\n",
      "Epoch 42: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9831 - loss: 0.0611 - val_accuracy: 1.0000 - val_loss: 0.2396 - learning_rate: 2.0000e-04\n",
      "Epoch 43/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0178\n",
      "Epoch 43: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.0321 - val_accuracy: 0.9583 - val_loss: 0.2174 - learning_rate: 2.0000e-04\n",
      "Epoch 44/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0386\n",
      "Epoch 44: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 1.0000 - loss: 0.0364 - val_accuracy: 0.9583 - val_loss: 0.2062 - learning_rate: 2.0000e-04\n",
      "Epoch 45/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0151\n",
      "Epoch 45: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9909 - loss: 0.0269 - val_accuracy: 0.9583 - val_loss: 0.1928 - learning_rate: 2.0000e-04\n",
      "Epoch 46/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0268\n",
      "Epoch 46: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0229 - val_accuracy: 0.9583 - val_loss: 0.1820 - learning_rate: 2.0000e-04\n",
      "Epoch 47/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9688 - loss: 0.0388\n",
      "Epoch 47: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9740 - loss: 0.0402 - val_accuracy: 1.0000 - val_loss: 0.1731 - learning_rate: 2.0000e-04\n",
      "Epoch 48/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0458\n",
      "Epoch 48: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.0358 - val_accuracy: 1.0000 - val_loss: 0.1597 - learning_rate: 2.0000e-04\n",
      "Epoch 49/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0085\n",
      "Epoch 49: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0147 - val_accuracy: 1.0000 - val_loss: 0.1494 - learning_rate: 2.0000e-04\n",
      "Epoch 50/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0391\n",
      "Epoch 50: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0324 - val_accuracy: 1.0000 - val_loss: 0.1398 - learning_rate: 2.0000e-04\n",
      "Epoch 51/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0085\n",
      "Epoch 51: val_accuracy did not improve from 1.00000\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0119 - val_accuracy: 1.0000 - val_loss: 0.1313 - learning_rate: 2.0000e-04\n",
      "Epoch 52/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0147\n",
      "Epoch 52: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0175 - val_accuracy: 1.0000 - val_loss: 0.1249 - learning_rate: 4.0000e-05\n",
      "Epoch 53/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0265\n",
      "Epoch 53: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0243 - val_accuracy: 1.0000 - val_loss: 0.1192 - learning_rate: 4.0000e-05\n",
      "Epoch 54/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0162\n",
      "Epoch 54: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0170 - val_accuracy: 1.0000 - val_loss: 0.1140 - learning_rate: 4.0000e-05\n",
      "Epoch 55/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0210\n",
      "Epoch 55: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0149 - val_accuracy: 1.0000 - val_loss: 0.1094 - learning_rate: 4.0000e-05\n",
      "Epoch 56/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0159\n",
      "Epoch 56: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0195 - val_accuracy: 1.0000 - val_loss: 0.1051 - learning_rate: 4.0000e-05\n",
      "Epoch 57/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0127\n",
      "Epoch 57: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0116 - val_accuracy: 1.0000 - val_loss: 0.1009 - learning_rate: 4.0000e-05\n",
      "Epoch 58/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0128\n",
      "Epoch 58: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0138 - val_accuracy: 1.0000 - val_loss: 0.0971 - learning_rate: 4.0000e-05\n",
      "Epoch 59/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0134\n",
      "Epoch 59: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 1.0000 - val_loss: 0.0936 - learning_rate: 4.0000e-05\n",
      "Epoch 60/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0110\n",
      "Epoch 60: val_accuracy did not improve from 1.00000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 0.0904 - learning_rate: 4.0000e-05\n",
      "Epoch 61/150\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0179\n",
      "Epoch 61: val_accuracy did not improve from 1.00000\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0191 - val_accuracy: 1.0000 - val_loss: 0.0874 - learning_rate: 4.0000e-05\n",
      "Epoch 61: early stopping\n",
      "Restoring model weights from the end of the best epoch: 41.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44a5c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.2752\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2a1f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as wsl_model_20250826-211017.h5\n"
     ]
    }
   ],
   "source": [
    "model.save(f'augment_main_less_actions_main_wsl_model_{timestamp}.h5')\n",
    "print(f\"\\nModel saved as wsl_model_{timestamp}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee576bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-26 21:11:31.988770: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756222892.178231   31821 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1756222892.187844   32196 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.1), renderer: llvmpipe (LLVM 19.1.1, 256 bits)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1756222892.231070   32112 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756222892.256641   32136 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756222892.258246   32120 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756222892.258741   32132 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756222892.258779   32135 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756222892.263181   32122 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756222892.269497   32116 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756222892.270699   32111 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1756222892.271205   32127 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/smayan/Desktop/AI-ML-DS/AI-and-ML-Course/.conda/lib/python3.11/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n",
      "res shape: (4,)\n",
      "argmax: 1\n"
     ]
    }
   ],
   "source": [
    "# Load trained model and label map\n",
    "model = tf.keras.models.load_model('/home/smayan/Desktop/ASL/no face/augment_main_less_actions_main_wsl_model_20250826-211017.h5')\n",
    "# label_map = np.load('/home/smayan/Desktop/ASL/no face/label_map.npy', allow_pickle=True).item()\n",
    "actions = list(label_map.keys())\n",
    "\n",
    "# Variables for prediction\n",
    "sequence = []\n",
    "sequence_length = 30\n",
    "threshold = 0.7\n",
    "\n",
    "# Start webcam / video\n",
    "cap = cv.VideoCapture('/home/smayan/Desktop/ASL/ssvid.net--breakfast-in-ASL_1080pFHR.mp4')\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize for consistency\n",
    "        frame = cv.resize(frame, (640, 480))\n",
    "\n",
    "        # Detection\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Extract keypoints (same as training)\n",
    "        keypoints = extract_keypoints(results)\n",
    "\n",
    "        # Append to sequence\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-sequence_length:]\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            input_seq = np.expand_dims(sequence, axis=0)   # shape (1, 30, 1530)\n",
    "\n",
    "            # Predict\n",
    "            res = model.predict(input_seq, verbose=0)[0]\n",
    "            print(\"res shape:\", res.shape)\n",
    "            print(\"argmax:\", np.argmax(res))\n",
    "\n",
    "            predicted_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "\n",
    "            # Show prediction if above threshold\n",
    "            if confidence > threshold:\n",
    "                cv.putText(image, f'{predicted_action}: {confidence:.2f}',\n",
    "                           (10, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Show probabilities\n",
    "            for i, (action, prob) in enumerate(zip(actions, res)):\n",
    "                y_pos = 100 + i * 30\n",
    "                cv.rectangle(image, (10, y_pos), (int(prob * 300) + 10, y_pos + 25), (0, 255, 0), -1)\n",
    "                cv.putText(image, f'{action}: {prob:.2f}', (15, y_pos + 18),\n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        # Show output\n",
    "        cv.imshow('ASL Inference', image)\n",
    "\n",
    "        # Quit\n",
    "        if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f5052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: (None, 20)\n",
      "Number of actions: 20\n",
      "Actions: ['hello', 'student', 'i', 'bye', 'goodbye', 'college', 'wrong', 'how', 'work', 'your', 'want', 'nice', 'to', 'meet', 'doctor', 'time', 'age', 'breakfast', 'sorry', 'love']\n"
     ]
    }
   ],
   "source": [
    "print(\"Model output shape:\", model.output_shape)  # e.g. (None, 10)\n",
    "print(\"Number of actions:\", len(actions))\n",
    "print(\"Actions:\", actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8bc39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
